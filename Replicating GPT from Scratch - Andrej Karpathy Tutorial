{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOjPtelkWiT6+4bymZOylPz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2zRuUdKYaMWI","executionInfo":{"status":"ok","timestamp":1719040749297,"user_tz":-480,"elapsed":566,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"7c5bac49-60f8-4a3e-a01a-6dadf7a4c2cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-06-22 07:19:07--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: ‘input.txt’\n","\n","\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.009s  \n","\n","2024-06-22 07:19:07 (119 MB/s) - ‘input.txt’ saved [1115394/1115394]\n","\n"]}],"source":["# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n","!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"]},{"cell_type":"code","source":["# '|'.join(['1','2'])"],"metadata":{"id":"jSYHWMc-QXqo","executionInfo":{"status":"ok","timestamp":1719040749298,"user_tz":-480,"elapsed":13,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# helper functions for better viz\n","def print_val_with_details(value,label,print_val=False,display_shape=True):\n","  vals_to_display = [f'Variable Name: {label}']\n","\n","  if \"shape\" in dir(value):\n","    if display_shape:\n","      vals_to_display.append(f'Shape: {value.shape}')\n","\n","  if print_val:\n","    vals_to_display.append(f'Value: {value}')\n","\n","  print(' | '.join(vals_to_display))\n","  return"],"metadata":{"id":"0B06PcvaPFdp","executionInfo":{"status":"ok","timestamp":1719040749298,"user_tz":-480,"elapsed":13,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# read it in to inspect it\n","with open('input.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()"],"metadata":{"id":"3BnDVbxFrHmP","executionInfo":{"status":"ok","timestamp":1719040749298,"user_tz":-480,"elapsed":13,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# let's look at the first 1000 characters\n","print(text[:1000])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Q2aj5xHrLNP","executionInfo":{"status":"ok","timestamp":1719040749298,"user_tz":-480,"elapsed":12,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"a8add678-e0c2-4148-96d2-d5ca4a2ecae4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n","All:\n","We know't, we know't.\n","\n","First Citizen:\n","Let us kill him, and we'll have corn at our own price.\n","Is't a verdict?\n","\n","All:\n","No more talking on't; let it be done: away, away!\n","\n","Second Citizen:\n","One word, good citizens.\n","\n","First Citizen:\n","We are accounted poor citizens, the patricians good.\n","What authority surfeits on would relieve us: if they\n","would yield us but the superfluity, while it were\n","wholesome, we might guess they relieved us humanely;\n","but they think we are too dear: the leanness that\n","afflicts us, the object of our misery, is as an\n","inventory to particularise their abundance; our\n","sufferance is a gain to them Let us revenge this with\n","our pikes, ere we become rakes: for the gods know I\n","speak this in hunger for bread, not in thirst for revenge.\n","\n","\n"]}]},{"cell_type":"code","source":["# here are all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","print(''.join(chars))\n","print(vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VXdMWQ4ZrOgR","executionInfo":{"status":"ok","timestamp":1719040749298,"user_tz":-480,"elapsed":10,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"b565aea6-9b46-4ee3-8bd2-43f497987d14"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n","65\n"]}]},{"cell_type":"code","source":["# create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","print(encode(\"hii there\"))\n","print(decode(encode(\"hii there\")))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7G20x-1KrQaX","executionInfo":{"status":"ok","timestamp":1719040749299,"user_tz":-480,"elapsed":8,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"0cb4e819-1f42-4ff8-be7b-f8b350f97c41"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[46, 47, 47, 1, 58, 46, 43, 56, 43]\n","hii there\n"]}]},{"cell_type":"code","source":["# let's now encode the entire text dataset and store it into a torch.Tensor\n","import torch # we use PyTorch: https://pytorch.org\n","data = torch.tensor(encode(text), dtype=torch.long)\n","print(data.shape, data.dtype)\n","print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Vht01HvsIkx","executionInfo":{"status":"ok","timestamp":1719040759162,"user_tz":-480,"elapsed":9870,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"28d1d74a-8b48-4b26-8a50-4d8ef9ff9158"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1115394]) torch.int64\n","tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n","        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n","         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n","        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n","         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n","        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n","         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n","        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n","        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n","         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n","         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n","        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n","        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n","         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n","        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n","        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n","        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n","        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n","        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n","        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n","        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n","         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n","         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n","         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n","        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n","        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n","        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n","        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n","        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n","        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n","        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n","         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n","         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n","        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n","        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n","        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n","         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n","        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n","        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n","         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n","        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n","        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n","        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n","        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n","        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n","        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n","        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n","        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n","        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n","         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n","        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n","        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n","        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n","        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n","        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n","        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"]}]},{"cell_type":"code","source":["# Let's now split up the data into train and validation sets\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]"],"metadata":{"id":"5QI8qCcksKir","executionInfo":{"status":"ok","timestamp":1719040759162,"user_tz":-480,"elapsed":10,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["block_size = 8\n","train_data[:block_size+1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TlzRkSUWsfPW","executionInfo":{"status":"ok","timestamp":1719040759162,"user_tz":-480,"elapsed":9,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"0aeae45e-fbec-4aa4-c985-a785842557f3"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["x = train_data[:block_size]\n","y = train_data[1:block_size+1]\n","for t in range(block_size):\n","    context = x[:t+1]\n","    target = y[t]\n","    print(f\"when input is {context} the target: {target}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pTFBJpU3EwRy","executionInfo":{"status":"ok","timestamp":1719040759162,"user_tz":-480,"elapsed":6,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"372fbe97-f1ae-4c37-f023-3338d85e2542"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["when input is tensor([18]) the target: 47\n","when input is tensor([18, 47]) the target: 56\n","when input is tensor([18, 47, 56]) the target: 57\n","when input is tensor([18, 47, 56, 57]) the target: 58\n","when input is tensor([18, 47, 56, 57, 58]) the target: 1\n","when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n","when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n","when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"]}]},{"cell_type":"code","source":["torch.manual_seed(1337)\n","batch_size = 4 # how many independent sequences will we process in parallel?\n","block_size = 8 # what is the maximum context length for predictions?\n","\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    return x, y\n","\n","xb, yb = get_batch('train')\n","print('inputs:')\n","print(xb.shape)\n","print(xb)\n","print('targets:')\n","print(yb.shape)\n","print(yb)\n","\n","print('----')\n","\n","for b in range(batch_size): # batch dimension\n","    for t in range(block_size): # time dimension\n","        context = xb[b, :t+1]\n","        target = yb[b,t]\n","        print(f\"when input is {context.tolist()} the target: {target}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gn2pXqIvEyYy","executionInfo":{"status":"ok","timestamp":1719040759701,"user_tz":-480,"elapsed":543,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"99a3b0e1-8a43-4447-c2ce-61bd0078e913"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs:\n","torch.Size([4, 8])\n","tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n","        [44, 53, 56,  1, 58, 46, 39, 58],\n","        [52, 58,  1, 58, 46, 39, 58,  1],\n","        [25, 17, 27, 10,  0, 21,  1, 54]])\n","targets:\n","torch.Size([4, 8])\n","tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n","        [53, 56,  1, 58, 46, 39, 58,  1],\n","        [58,  1, 58, 46, 39, 58,  1, 46],\n","        [17, 27, 10,  0, 21,  1, 54, 39]])\n","----\n","when input is [24] the target: 43\n","when input is [24, 43] the target: 58\n","when input is [24, 43, 58] the target: 5\n","when input is [24, 43, 58, 5] the target: 57\n","when input is [24, 43, 58, 5, 57] the target: 1\n","when input is [24, 43, 58, 5, 57, 1] the target: 46\n","when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n","when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n","when input is [44] the target: 53\n","when input is [44, 53] the target: 56\n","when input is [44, 53, 56] the target: 1\n","when input is [44, 53, 56, 1] the target: 58\n","when input is [44, 53, 56, 1, 58] the target: 46\n","when input is [44, 53, 56, 1, 58, 46] the target: 39\n","when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n","when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n","when input is [52] the target: 58\n","when input is [52, 58] the target: 1\n","when input is [52, 58, 1] the target: 58\n","when input is [52, 58, 1, 58] the target: 46\n","when input is [52, 58, 1, 58, 46] the target: 39\n","when input is [52, 58, 1, 58, 46, 39] the target: 58\n","when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n","when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n","when input is [25] the target: 17\n","when input is [25, 17] the target: 27\n","when input is [25, 17, 27] the target: 10\n","when input is [25, 17, 27, 10] the target: 0\n","when input is [25, 17, 27, 10, 0] the target: 21\n","when input is [25, 17, 27, 10, 0, 21] the target: 1\n","when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n","when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"]}]},{"cell_type":"code","source":["xb, yb = get_batch('train')\n","xb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rs74RHMQLFg4","executionInfo":{"status":"ok","timestamp":1719040759702,"user_tz":-480,"elapsed":8,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"0a8d44ce-cb6f-4c82-80ae-b91d58194498"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[57, 43, 60, 43, 52,  1, 63, 43],\n","        [60, 43, 42,  8,  0, 25, 63,  1],\n","        [56, 42,  5, 57,  1, 57, 39, 49],\n","        [43, 57, 58, 63,  6,  1, 58, 46]])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["xb.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yVOaO__qGZx7","executionInfo":{"status":"ok","timestamp":1719040759702,"user_tz":-480,"elapsed":6,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"1f18c5b2-c0f3-4bff-8439-7d9ac43cff99"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","torch.manual_seed(1337)\n","\n","class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        logits = self.token_embedding_table(idx) # (B,T,C)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","        # print_val_with_details(logits,'logits_first')\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # get the predictions\n","            logits, loss = self(idx)\n","            # print_val_with_details(logits,'logits_first')\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n"],"metadata":{"id":"Hl_fKRGWHOra","executionInfo":{"status":"ok","timestamp":1719040759702,"user_tz":-480,"elapsed":4,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["m = BigramLanguageModel(vocab_size)\n","logits, loss = m(xb, yb)\n","print(logits.shape)\n","print(loss)\n","\n","print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qrx6haPbe1Mq","executionInfo":{"status":"ok","timestamp":1719040759702,"user_tz":-480,"elapsed":4,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"eedba4a7-38d0-4e77-dc68-e91df39655d5"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 65])\n","tensor(4.4150, grad_fn=<NllLossBackward0>)\n","\n","Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"]}]},{"cell_type":"code","source":["# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"],"metadata":{"id":"qpbUoZbhLrpK","executionInfo":{"status":"ok","timestamp":1719040763648,"user_tz":-480,"elapsed":3949,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","for steps in range(1000): # increase number of steps for good results...\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = m(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","print(loss.item())\n"],"metadata":{"id":"wMpJJ9DtSoNE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719040766807,"user_tz":-480,"elapsed":3162,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"086b0025-b62f-4c39-d6a4-30e402321f31"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["3.7218432426452637\n"]}]},{"cell_type":"code","source":["print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vbPLwLnYuC5c","executionInfo":{"status":"ok","timestamp":1719040766807,"user_tz":-480,"elapsed":25,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"28f30147-c519-4ca2-84dc-22402abc0bca"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","olylvLLko'TMyatyIoconxad.?-tNSqYPsx&bF.oiR;BD$dZBMZv'K f bRSmIKptRPly:AUC&$zLK,qUEy&Ay;ZxjKVhmrdagC-bTop-QJe.H?x\n","JGF&pwst-P sti.hlEsu;w:w a BG:tLhMk,epdhlay'sVzLq--ERwXUzDnq-bn czXxxI&V&Pynnl,s,Ioto!uvixwC-IJXElrgm C-.bcoCPJ\n","IMphsevhO AL!-K:AIkpre,\n","rPHEJUzV;P?uN3b?ohoRiBUENoV3B&jumNL;Aik,\n","xf -IEKROn JSyYWW?n 'ay;:weO'AqVzPyoiBL? seAX3Dot,iy.xyIcf r!!ul-Koi:x pZrAQly'v'a;vEzN\n","BwowKo'MBqF$PPFb\n","CjYX3beT,lZ qdda!wfgmJP\n","DUfNXmnQU mvcv?nlnQF$JUAAywNocd  bGSPyAlprNeQnq-GRSVUP.Ja!IBoDqfI&xJM AXEHV&DKvRS\n"]}]},{"cell_type":"markdown","source":["## trick to self attention or how to compute the parts of it"],"metadata":{"id":"YgHlI6vK4asz"}},{"cell_type":"code","source":["# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n","torch.manual_seed(42)\n","a = torch.tril(torch.ones(3, 3))\n","a = a / torch.sum(a, 1, keepdim=True)\n","b = torch.randint(0,10,(3,2)).float()\n","c = a @ b\n","print('a=')\n","print(a)\n","print('--')\n","print('b=')\n","print(b)\n","print('--')\n","print('c=')\n","print(c)"],"metadata":{"id":"wJ4FibxOsfiW","executionInfo":{"status":"ok","timestamp":1719040766807,"user_tz":-480,"elapsed":22,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5d4d0594-de5d-4edf-d81d-41e585149c83"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["a=\n","tensor([[1.0000, 0.0000, 0.0000],\n","        [0.5000, 0.5000, 0.0000],\n","        [0.3333, 0.3333, 0.3333]])\n","--\n","b=\n","tensor([[2., 7.],\n","        [6., 4.],\n","        [6., 5.]])\n","--\n","c=\n","tensor([[2.0000, 7.0000],\n","        [4.0000, 5.5000],\n","        [4.6667, 5.3333]])\n"]}]},{"cell_type":"code","source":["# consider the following toy example:\n","\n","torch.manual_seed(1337)\n","B,T,C = 4,8,2 # batch, time, channels\n","x = torch.randn(B,T,C)\n","x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A13nCP0V56MF","executionInfo":{"status":"ok","timestamp":1719040766807,"user_tz":-480,"elapsed":20,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"4f79178a-eeb7-4f5f-c104-f2a7e0aa025a"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 2])"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["# version 1\n","# We want x[b,t] = mean_{i<=t} x[b,i]\n","# Carl's Note: Objective of form we want to get\n","xbow = torch.zeros((B,T,C))\n","for b in range(B):\n","    for t in range(T):\n","        xprev = x[b,:t+1] # (t,C)\n","        xbow[b,t] = torch.mean(xprev, 0)\n"],"metadata":{"id":"fiUXfZjFsLtp","executionInfo":{"status":"ok","timestamp":1719040766807,"user_tz":-480,"elapsed":18,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# version 2: using matrix multiply for a weighted aggregation, which is a more efficient way to do than version 1\n","wei = torch.tril(torch.ones(T, T))\n","wei = wei / wei.sum(1, keepdim=True)\n","xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n","torch.allclose(xbow, xbow2) # check if they are the same as the results above"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tdwhtYRXr8xa","executionInfo":{"status":"ok","timestamp":1719040766808,"user_tz":-480,"elapsed":18,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"4b244869-9fec-4a17-d09a-bb1824b68dfc"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["wei"],"metadata":{"id":"DE8PlWhysBn3","executionInfo":{"status":"ok","timestamp":1719040767509,"user_tz":-480,"elapsed":718,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"396f0779-870e-44d6-a613-39c527287511"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n","        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n","        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n","        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["type(xbow2[3][-1][-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-6Xoaatd6FhP","executionInfo":{"status":"ok","timestamp":1719040767509,"user_tz":-480,"elapsed":15,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"0b97f127-1e62-41fd-ffaf-032f19031ab8"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Tensor"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["type(xbow[3][-1][-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TWh-_VJi6Gh-","executionInfo":{"status":"ok","timestamp":1719040767509,"user_tz":-480,"elapsed":13,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"8081b7ab-a02e-47a0-9a5d-55c4c98a37b9"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Tensor"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["# version 3: use Softmax\n","tril = torch.tril(torch.ones(T, T))\n","wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril == 0, float('-inf'))\n","wei = F.softmax(wei, dim=-1)\n","xbow3 = wei @ x\n","torch.allclose(xbow, xbow3)\n"],"metadata":{"id":"WuO13Tiz61v7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719040767510,"user_tz":-480,"elapsed":12,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"32cf18fa-1f0a-496b-9ecd-7aacd316bab6"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["## Current Training Script for Bigram Model"],"metadata":{"id":"Bw9MI9mOrLjL"}},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yOeI7whmrc6W","executionInfo":{"status":"ok","timestamp":1719040767510,"user_tz":-480,"elapsed":9,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"7b312f8e-12ae-4eb4-8fa6-2dc9b8c09afb"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-06-22 07:19:26--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: ‘input.txt.1’\n","\n","\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n","\n","2024-06-22 07:19:26 (95.8 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n","\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","RUN_SCRIPT = True\n","START_TRAIN = False\n","\n","if RUN_SCRIPT:\n","  # hyperparameters\n","  batch_size = 32 # how many independent sequences will we process in parallel?\n","  block_size = 8 # what is the maximum context length for predictions?\n","  max_iters = 3000\n","  eval_interval = 300\n","  learning_rate = 1e-2\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  eval_iters = 200\n","  n_embd = 32\n","  # ------------\n","\n","  torch.manual_seed(1337)\n","\n","\n","  with open('input.txt', 'r', encoding='utf-8') as f:\n","      text = f.read()\n","\n","  # here are all the unique characters that occur in this text\n","  chars = sorted(list(set(text)))\n","  vocab_size = len(chars)\n","  # create a mapping from characters to integers\n","  stoi = { ch:i for i,ch in enumerate(chars) }\n","  itos = { i:ch for i,ch in enumerate(chars) }\n","  encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","  decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","  # Train and test splits\n","  data = torch.tensor(encode(text), dtype=torch.long)\n","  n = int(0.9*len(data)) # first 90% will be train, rest val\n","  train_data = data[:n]\n","  val_data = data[n:]\n","\n","  # data loading\n","  def get_batch(split):\n","      # generate a small batch of data of inputs x and targets y\n","      data = train_data if split == 'train' else val_data\n","      ix = torch.randint(len(data) - block_size, (batch_size,))\n","      x = torch.stack([data[i:i+block_size] for i in ix])\n","      y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","      x, y = x.to(device), y.to(device)\n","      return x, y\n","\n","  @torch.no_grad()\n","  def estimate_loss():\n","      out = {}\n","      model.eval()\n","      for split in ['train', 'val']:\n","          losses = torch.zeros(eval_iters)\n","          for k in range(eval_iters):\n","              X, Y = get_batch(split)\n","              logits, loss = model(X, Y)\n","              losses[k] = loss.item()\n","          out[split] = losses.mean()\n","      model.train()\n","      return out\n","\n","  # super simple bigram model\n","  class BigramLanguageModel(nn.Module):\n","\n","      def __init__(self):\n","          super().__init__()\n","          # each token directly reads off the logits for the next token from a lookup table\n","          self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","          self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","          self.lm_head = nn.Linear(n_embd, vocab_size)\n","          self.token_embed_sample = None\n","\n","      def forward(self, idx, targets=None):\n","          B, T = idx.shape\n","\n","          # idx and targets are both (B,T) tensor of integers\n","          tok_emb = self.token_embedding_table(idx) # (B,T,C)\n","          self.token_embed_sample = tok_emb\n","          pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n","          x = tok_emb + pos_emb # (B,T,C)\n","          logits = self.lm_head(x) # (B,T,vocab_size)\n","\n","          if targets is None:\n","              loss = None\n","          else:\n","              B, T, C = logits.shape\n","              logits = logits.view(B*T, C)\n","              targets = targets.view(B*T)\n","              loss = F.cross_entropy(logits, targets)\n","\n","          return logits, loss\n","\n","      def generate(self, idx, max_new_tokens):\n","          # idx is (B, T) array of indices in the current context\n","          for _ in range(max_new_tokens):\n","              # get the predictions\n","              logits, loss = self(idx)\n","              # focus only on the last time step\n","              logits = logits[:, -1, :] # becomes (B, C)\n","              # apply softmax to get probabilities\n","              probs = F.softmax(logits, dim=-1) # (B, C)\n","              # sample from the distribution\n","              idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","              # append sampled index to the running sequence\n","              idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","          return idx\n","\n","  model = BigramLanguageModel()\n","  m = model.to(device)\n","\n","  # create a PyTorch optimizer\n","  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","  if START_TRAIN:\n","    for iter in range(max_iters):\n","\n","        # every once in a while evaluate the loss on train and val sets\n","        if iter % eval_interval == 0:\n","            losses = estimate_loss()\n","            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","        # sample a batch of data\n","        xb, yb = get_batch('train')\n","\n","        # evaluate the loss\n","        logits, loss = model(xb, yb)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss.backward()\n","        optimizer.step()\n","\n","  # generate from the model\n","  context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","  a = m.generate(context, max_new_tokens=500)\n","\n","  # print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"],"metadata":{"id":"91fnc5FIrN1-","executionInfo":{"status":"error","timestamp":1719040768022,"user_tz":-480,"elapsed":517,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"colab":{"base_uri":"https://localhost:8080/","height":373},"outputId":"4c9fa6d4-95b3-4a53-aa82-86e92a56798e"},"execution_count":29,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"index out of range in self","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-a8758817714f>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0;31m# generate from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m   \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0;31m# print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-a8758817714f>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[1;32m     98\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m               \u001b[0;31m# get the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m               \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m               \u001b[0;31m# focus only on the last time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m               \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# becomes (B, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-a8758817714f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     80\u001b[0m           \u001b[0mtok_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embed_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_emb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m           \u001b[0mpos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m           \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_emb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_emb\u001b[0m \u001b[0;31m# (B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m           \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,T,vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2262\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index out of range in self"]}]},{"cell_type":"code","source":["weights = torch.tensor([.9,.50,.60,.01,.99], dtype=torch.float)"],"metadata":{"id":"INOualzzzk03","executionInfo":{"status":"aborted","timestamp":1719040768023,"user_tz":-480,"elapsed":11,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# weights = torch.tensor([]*6, dtype=torch.float)\n","out = torch.multinomial(weights, num_samples=1)\n","# out_count = out.unique(return_counts=True)[1]"],"metadata":{"id":"rZVR0vRsmnEs","executionInfo":{"status":"aborted","timestamp":1719040768023,"user_tz":-480,"elapsed":10,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out"],"metadata":{"id":"4xbwgI6GzW5O","executionInfo":{"status":"aborted","timestamp":1719040768023,"user_tz":-480,"elapsed":10,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["m(get_batch('train')[0])"],"metadata":{"id":"BIRFUx_0jtfw","executionInfo":{"status":"aborted","timestamp":1719040768023,"user_tz":-480,"elapsed":10,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create a single attention head"],"metadata":{"id":"WogSGL4Btcn4"}},{"cell_type":"code","source":["# version 4: self-attention!\n","torch.manual_seed(1337)\n","B,T,C = 4,8,32 # batch, time, channels\n","x = torch.randn(B,T,C)\n","\n","# let's see a single Head perform self-attention\n","head_size = 16\n","key = nn.Linear(C, head_size, bias=False)\n","query = nn.Linear(C, head_size, bias=False)\n","value = nn.Linear(C, head_size, bias=False)\n","k = key(x)   # (B, T, 16)\n","q = query(x) # (B, T, 16)\n","wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n","\n","tril = torch.tril(torch.ones(T, T))\n","#wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril == 0, float('-inf'))\n","# print_val_with_details(wei[0],'1',print_val=True)\n","wei = F.softmax(wei, dim=-1)\n","# print_val_with_details(wei,'2',print_val=True)\n","\n","v = value(x)\n","# out = wei @ x\n","out = wei @ v\n","\n","out.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iI6eTCSetfAj","executionInfo":{"status":"ok","timestamp":1718727661274,"user_tz":-480,"elapsed":327,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"93d99aac-c61a-4d4e-b4ca-51c33c435357"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 32])"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["wei[2].sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DFs2KINkuN_x","executionInfo":{"status":"ok","timestamp":1718727638455,"user_tz":-480,"elapsed":335,"user":{"displayName":"Carl John Vinas","userId":"04069169716695320653"}},"outputId":"7b26fe00-7985-4165-ccc6-244acadd8d6e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.0000, grad_fn=<SumBackward0>)"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":[],"metadata":{"id":"jlRAUrHq1-i8"},"execution_count":null,"outputs":[]}]}